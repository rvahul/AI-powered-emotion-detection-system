{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install Requirements"
      ],
      "metadata": {
        "id": "jSXKmRbwts4O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WGHzsL9tpsG"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python-headless tensorflow keras librosa transformers streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Facial Emotion Recognition (OpenCV + CNN)"
      ],
      "metadata": {
        "id": "r_Au4FfktxP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "model = load_model('emotion_model.h5')  # Pretrained model (FER2013)\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "def detect_emotion_from_image(image_path):\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        roi = gray[y:y+h, x:x+w]\n",
        "        roi = cv2.resize(roi, (48, 48))\n",
        "        roi = roi.astype(\"float\") / 255.0\n",
        "        roi = np.expand_dims(roi, axis=0)\n",
        "        roi = np.expand_dims(roi, axis=-1)\n",
        "\n",
        "        prediction = model.predict(roi)\n",
        "        emotion = emotion_labels[np.argmax(prediction)]\n",
        "        return emotion\n",
        "    return \"No Face Detected\""
      ],
      "metadata": {
        "id": "ADIhoXR8tz08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Voice Emotion Detection (Librosa + SVM)"
      ],
      "metadata": {
        "id": "krgxGornt5rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "model = joblib.load('voice_emotion_model.pkl')  # Pretrained SVM\n",
        "\n",
        "def extract_features(file):\n",
        "    y, sr = librosa.load(file)\n",
        "    features = np.hstack([\n",
        "        np.mean(librosa.feature.mfcc(y, sr), axis=1),\n",
        "        np.mean(librosa.feature.chroma_stft(y, sr), axis=1),\n",
        "        np.mean(librosa.feature.melspectrogram(y, sr), axis=1)\n",
        "    ])\n",
        "    return features.reshape(1, -1)\n",
        "\n",
        "def detect_emotion_from_voice(file):\n",
        "    features = extract_features(file)\n",
        "    emotion = model.predict(features)[0]\n",
        "    return emotion"
      ],
      "metadata": {
        "id": "tFV3A3cBt8E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Text Sentiment Detection (BERT or VADER)"
      ],
      "metadata": {
        "id": "j8yOO4ppt_NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text_emotion.py\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def detect_emotion_from_text(text):\n",
        "    result = classifier(text)[0]\n",
        "    return f\"{result['label']} ({round(result['score']*100, 2)}%)\""
      ],
      "metadata": {
        "id": "7Of61yXiuBgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. GUI and Multi-Modal Fusion (Streamlit)"
      ],
      "metadata": {
        "id": "Cl55sKL6uDvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py\n",
        "import streamlit as st\n",
        "from facial_emotion import detect_emotion_from_image\n",
        "from voice_emotion import detect_emotion_from_voice\n",
        "from text_emotion import detect_emotion_from_text\n",
        "\n",
        "st.title(\"ðŸŽ­ Multi-Modal Emotion Detection System\")\n",
        "\n",
        "tab = st.sidebar.radio(\"Select Input Type\", [\"Image\", \"Voice\", \"Text\"])\n",
        "\n",
        "if tab == \"Image\":\n",
        "    img = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"png\"])\n",
        "    if img:\n",
        "        with open(\"temp.jpg\", \"wb\") as f:\n",
        "            f.write(img.read())\n",
        "        emotion = detect_emotion_from_image(\"temp.jpg\")\n",
        "        st.success(f\"Detected Emotion: {emotion}\")\n",
        "\n",
        "elif tab == \"Voice\":\n",
        "    audio = st.file_uploader(\"Upload a voice file\", type=[\"wav\", \"mp3\"])\n",
        "    if audio:\n",
        "        with open(\"temp.wav\", \"wb\") as f:\n",
        "            f.write(audio.read())\n",
        "        emotion = detect_emotion_from_voice(\"temp.wav\")\n",
        "        st.success(f\"Detected Emotion: {emotion}\")\n",
        "\n",
        "elif tab == \"Text\":\n",
        "    text = st.text_input(\"Enter text:\")\n",
        "    if text:\n",
        "        emotion = detect_emotion_from_text(text)\n",
        "        st.success(f\"Sentiment Analysis: {emotion}\")"
      ],
      "metadata": {
        "id": "lMVNns3auFqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Notes:\n",
        "You'll need a pretrained facial emotion model (emotion_model.h5) and voice emotion SVM model (voice_emotion_model.pkl). I can help you train or find them.\n",
        "\n",
        "This is a simple prototype â€” for fusion, you could combine predictions using majority voting or a meta-classifier."
      ],
      "metadata": {
        "id": "XIgCmsvzuLeb"
      }
    }
  ]
}